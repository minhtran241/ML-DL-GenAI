{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Advanced Technique: Reranking with Cohere AI\n",
    "\n",
    "Reranking is a technique used to improve the performance of a model by reordering the results of the model. It works by following these steps:\n",
    "\n",
    "1. Generate a list of candidate answers using a model.\n",
    "2. Score each candidate answer using a reranker model.\n",
    "3. Reorder the candidate answers based on the scores.\n",
    "4. Return the top candidate answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    datasets \\\n",
    "    pinecone-client \\\n",
    "    cohere==4.34\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5016af710c8044569f645758d5d9992e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/153M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6301d401c84216aa11a788a105c6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/41584 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references'],\n",
       "    num_rows: 41584\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"jamescalam/ai-arxiv-chunked\", split=\"train\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doi': '1910.01108',\n",
       " 'chunk-id': '0',\n",
       " 'chunk': 'DistilBERT, a distilled version of BERT: smaller,\\nfaster, cheaper and lighter\\nVictor SANH, Lysandre DEBUT, Julien CHAUMOND, Thomas WOLF\\nHugging Face\\n{victor,lysandre,julien,thomas}@huggingface.co\\nAbstract\\nAs Transfer Learning from large-scale pre-trained models becomes more prevalent\\nin Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains\\nchallenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be ﬁnetuned with good performances on a wide range of tasks like its larger counterparts.\\nWhile most prior work investigated the use of distillation for building task-speciﬁc\\nmodels, we leverage knowledge distillation during the pre-training phase and show\\nthat it is possible to reduce the size of a BERT model by 40%, while retaining 97%\\nof its language understanding capabilities and being 60% faster. To leverage the\\ninductive biases learned by larger models during pre-training, we introduce a triple\\nloss combining language modeling, distillation and cosine-distance losses. Our\\nsmaller, faster and lighter model is cheaper to pre-train and we demonstrate its',\n",
       " 'id': '1910.01108',\n",
       " 'title': 'DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter',\n",
       " 'summary': 'As Transfer Learning from large-scale pre-trained models becomes more\\nprevalent in Natural Language Processing (NLP), operating these large models in\\non-the-edge and/or under constrained computational training or inference\\nbudgets remains challenging. In this work, we propose a method to pre-train a\\nsmaller general-purpose language representation model, called DistilBERT, which\\ncan then be fine-tuned with good performances on a wide range of tasks like its\\nlarger counterparts. While most prior work investigated the use of distillation\\nfor building task-specific models, we leverage knowledge distillation during\\nthe pre-training phase and show that it is possible to reduce the size of a\\nBERT model by 40%, while retaining 97% of its language understanding\\ncapabilities and being 60% faster. To leverage the inductive biases learned by\\nlarger models during pre-training, we introduce a triple loss combining\\nlanguage modeling, distillation and cosine-distance losses. Our smaller, faster\\nand lighter model is cheaper to pre-train and we demonstrate its capabilities\\nfor on-device computations in a proof-of-concept experiment and a comparative\\non-device study.',\n",
       " 'source': 'http://arxiv.org/pdf/1910.01108',\n",
       " 'authors': ['Victor Sanh',\n",
       "  'Lysandre Debut',\n",
       "  'Julien Chaumond',\n",
       "  'Thomas Wolf'],\n",
       " 'categories': ['cs.CL'],\n",
       " 'comment': 'February 2020 - Revision: fix bug in evaluation metrics, updated\\n  metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at\\n  the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing\\n  - NeurIPS 2019',\n",
       " 'journal_ref': None,\n",
       " 'primary_category': 'cs.CL',\n",
       " 'published': '20191002',\n",
       " 'updated': '20200301',\n",
       " 'references': [{'id': '1910.01108'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f4ae709228468c847f28a021250f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41584 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['doi', 'chunk-id', 'chunk', 'id', 'title', 'summary', 'source', 'authors', 'categories', 'comment', 'journal_ref', 'primary_category', 'published', 'updated', 'references', 'text', 'metadata'],\n",
       "    num_rows: 41584\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.map(lambda x: {\n",
    "\t\"id\": f'{x[\"id\"]}-{x[\"chunk-id\"]}',\n",
    "\t\"text\": x[\"chunk\"],\n",
    "\t\"metadata\": {\n",
    "\t\t\"title\": x[\"title\"],\n",
    "\t\t\"url\": x[\"source\"],\n",
    "\t\t\"primary_category\": x[\"primary_category\"],\n",
    "\t\t\"published\": x[\"published\"],\n",
    "\t\t\"updated\": x[\"updated\"],\n",
    "\t\t\"text\": x[\"chunk\"]\n",
    "\t}\n",
    "})\n",
    "# Drop unnecessary columns\n",
    "data.remove_columns([\n",
    "\t\"title\", \"summary\", \"source\", \"authors\", \"categories\", \"comment\", \"journal_ref\", \"primary_category\", \"published\", \"updated\", \"references\", \"doi\", \"chunk-id\", \"chunk\"\n",
    "])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Embedding Function and DB Connection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Embedding Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cohere\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_api_key = os.getenv(\"COHERE_API_KEY\") or getpass.getpass(\"Enter your Cohere API key: \")\n",
    "co = cohere.Client(api_key=co_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(docs: list[str], input_type: str = \"search_document\") -> list[list[float]]:\n",
    "    doc_embeds = co.embed(\n",
    "            texts=docs,\n",
    "        input_type=input_type,\n",
    "        model=\"embed-english-v3.0\"\n",
    "    )\n",
    "    return doc_embeds.embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the DB Connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Pinecone connection with your API key\n",
    "pc_api_key = os.getenv(\"PINECONE_API_KEY\") or getpass.getpass(\"Enter your Pinecone API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(api_key=pc_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"arxiv-rerankers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': []}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.list_indexes()\n",
    "# pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the index exists\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    # If does not exist, create a new index\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1024, # Replace with your model dimensions\n",
    "        metric=\"cosine\", # Replace with your model metric\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        ) \n",
    "    )\n",
    "    # wait for the index to be created\n",
    "    while index_name not in pc.list_indexes().names():\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1024,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# View index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populating the index with the Cohere's `embed-english-v3.0` model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100 # How many embeddings we create and insert at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0, len(data), BATCH_SIZE)):\n",
    "    passed = False\n",
    "    # find end of batch\n",
    "    i_end = min(len(data), i+BATCH_SIZE)\n",
    "    # create batch\n",
    "    batch = data[i:i_end]\n",
    "    embeds = embed(batch[\"text\"])\n",
    "    to_upsert = list(zip(batch[\"id\"], embeds, batch[\"metadata\"]))\n",
    "    # upsert to Pinecone\n",
    "    index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test retrieval without Cohere's reranker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(query: str, top_k: int = 5) -> dict:\n",
    "\t# Embed query\n",
    "\tquery_embed = embed([query], input_type=\"search_query\")[0]\n",
    "\t# Search Pinecone\n",
    "\tresults = index.query(vector=query_embed, top_k=top_k, include_metadata=True)\n",
    "\t# Get the docs\n",
    "\tdocs = {x[\"metadata\"][\"text\"]: i for i, x in enumerate(results[\"matches\"])}\n",
    "\treturn docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['preferences and values which are diﬃcult to capture by hard- coded reward functions.\\nRLHF works by using a pre-trained LM to generate text, which i s then evaluated by humans by, for example,\\nranking two model generations for the same prompt. This data is then collected to learn a reward model\\nthat predicts a scalar reward given any generated text. The r eward captures human preferences when\\njudging model output. Finally, the LM is optimized against s uch reward model using RL policy gradient\\nalgorithms like PPO ( Schulman et al. ,2017). RLHF can be applied directly on top of a general-purpose LM\\npre-trained via self-supervised learning. However, for mo re complex tasks, the model’s generations may not\\nbe good enough. In such cases, RLHF is typically applied afte r an initial supervised ﬁne-tuning phase using\\na small number of expert demonstrations for the correspondi ng downstream task ( Ramamurthy et al. ,2022;\\nOuyang et al. ,2022;Stiennon et al. ,2020).\\nA successful example of RLHF used to teach a LM to use an extern al tool stems from WebGPT Nakano et al.\\n(2021) (discussed in 3.2.3), a model capable of answering questions using a search engine and providing',\n",
       " 'the context, and model-written functions are run in a sandbox environment. In Figure 21 we show results\\nversus model size with and without RLHF training. We see the same trend here as with other evaluations –\\nRLHF decreases the performance of small models, but improves the performance of larger models.\\nRL training tends to decrease the entropy of the models’ distribution, and so we were concerned that these\\nresults would be very sensitive to temperature and top-p tuning. So for our 52B models, we performed a\\nscan over temperatures and two top-p settings for both the RLHF models and the base code models, and then\\nchose the best setting for each model and pass@k . We did a grid-search over the evaluation hyperparameters:\\nT2f0;0:4;0:6;0:8;1:0g\\x02p2f0:95;1g\\x02k2f1;5;10;25;50;75;100g. Results are summarized on the\\nright side of Figure 21. For each model and for each kinpass@k , we take the maximum performance over\\nall 10 combinations of hyperparameters. We see that RLHF improves performance over the baseline on this\\nevaluation, for all pass@k .\\nWe should emphasize that as with our other evaluations, the improvements in performance from RLHF are',\n",
       " 'evaluation, for all pass@k .\\nWe should emphasize that as with our other evaluations, the improvements in performance from RLHF are\\nmodest. In fact, we ﬁnd that simply prompting a base code model performs slightly better, as shown in Figure\\n26\\n1071081091010\\nNumber of Parameters0.000.050.100.150.200.250.30Accuracy (pass@1)\\nHumanEval Performance\\nPython FT + RLHF \\nPython FT \\n100101102\\nk in pass@k0.40.50.60.70.80.9Accuracy\\nEffect of RLHF on HumanEval Performance for 52B Models\\nPython FT + RLHF \\nPython FT Figure 21 (left) Pass@1 accuracy of base code models and RLHF models on HumanEval. RLHF generally\\ndecreases performance on smaller models, but improves performance on larger models. (right) This ﬁgure\\nshows performance of our 52B models as a function of kfor Pass@k. We did a grid-search over the evaluation\\nhyperparameters T2f0;0:4;0:6;0:8;1:0g\\x02p2f0:95;1g, and plotted the maximum accuracy at each k.\\nResults show that RLHF actually improves performance, even at large k.',\n",
       " 'preferences in order to make it more useful. One key component of RLHF is reward modeling,\\nwhere the problem is formulated as a regression task to predict a scalar reward given a prompt and\\na response (Askell et al., 2021; Ouyang et al., 2022). This approach typically requires large-scale\\ncomparison data, where two model responses on the same prompt are compared Ouyang et al.\\n(2022). Existing open-source works such as Alpaca, Vicuna, and Dolly (Databricks, 2023) do not\\ninvolve RLHF due to the high cost of labeling comparison data. Meanwhile, recent studies show that\\nGPT-4 is capable of identifying and ﬁxing its own mistakes, and accurately judging the quality of\\nresponses(Peng et al., 2023; Bai et al., 2022; Madaan et al., 2023; Kim et al., 2023). Therefore, to\\nfacilitate research on RLHF, we have created comparison data using GPT-4, as described in Section 2.\\nFigure 2: The distribution of comparison\\ndata.To evaluate data quality, we train a reward model based\\non OPT 1.3B (Iyer et al., 2022) to rate different responses. For each instance of the comparison data involving one prompt xandKresponses, GPT-4 assigns a',\n",
       " 'size of 1024 tokens, except for the ‘online’ model described in Section 4.5, where we trained with 2048,\\nwhich may help stabilize RLHF on long contexts.\\nFor both PMP and human feedback ﬁnetuning, we append a special ‘end-of-context’ token at the end of each\\nsample, such that the PM score is predicted directly on top of this token. As explained in Appendix C.4 of\\n[Askell et al., 2021], this appears to improve PM performance.\\n40\\n1071081091010\\nNumber of Parameters0.10.20.30.40.50.60.70.8Accuracy\\nZero-Shot Accuracy on Lambada\\nPlain Language Model\\nRLHF\\n1071081091010\\nNumber of Parameters0.30.40.50.60.7Accuracy\\nZero-Shot Accuracy on ARC-Easy\\nPlain Language Model\\nRLHF\\n1071081091010\\nNumber of Parameters0.250.300.350.400.450.500.550.60Accuracy\\nZero-Shot Accuracy on ARC-Challenge\\nPlain Language Model\\nRLHF\\n1071081091010\\nNumber of Parameters0.2500.2750.3000.3250.3500.3750.4000.425Accuracy\\nZero-Shot Accuracy on MMLU\\nPlain Language Model\\nRLHF']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Can you explain why we would want to do rlhf?\"\n",
    "docs = get_docs(query, top_k=25)\n",
    "list(docs.keys())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking Responses\n",
    "\n",
    "We can easily get the responses we need when include _many_ responses, but this doesn't work well with LLMs. The recall performance for LLMs **decrease as we add more into the context window** - we call this excessive filling of the context window _\"context stuffing\"_.\n",
    "\n",
    "Fortunately reranking offers us a solution that helps us find those records that may not be within the top-3 results, and pull them into a smaller set of results to be given to the LLM.\n",
    "\n",
    "We will use Cohere's rerank endpoint for this purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "rerank_docs = co.rerank(\n",
    "\tquery=query,\n",
    "\tdocuments=list(docs.keys()),\n",
    "\ttop_n=25,\n",
    "    model=\"rerank-english-v3.0\",\n",
    "\treturn_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RerankResponseResultsItem(document=RerankResponseResultsItemDocument(text='preferences in order to make it more useful. One key component of RLHF is reward modeling,\\nwhere the problem is formulated as a regression task to predict a scalar reward given a prompt and\\na response (Askell et al., 2021; Ouyang et al., 2022). This approach typically requires large-scale\\ncomparison data, where two model responses on the same prompt are compared Ouyang et al.\\n(2022). Existing open-source works such as Alpaca, Vicuna, and Dolly (Databricks, 2023) do not\\ninvolve RLHF due to the high cost of labeling comparison data. Meanwhile, recent studies show that\\nGPT-4 is capable of identifying and ﬁxing its own mistakes, and accurately judging the quality of\\nresponses(Peng et al., 2023; Bai et al., 2022; Madaan et al., 2023; Kim et al., 2023). Therefore, to\\nfacilitate research on RLHF, we have created comparison data using GPT-4, as described in Section 2.\\nFigure 2: The distribution of comparison\\ndata.To evaluate data quality, we train a reward model based\\non OPT 1.3B (Iyer et al., 2022) to rate different responses. For each instance of the comparison data involving one prompt xandKresponses, GPT-4 assigns a'), index=3, relevance_score=0.99253935),\n",
       " RerankResponseResultsItem(document=RerankResponseResultsItemDocument(text='preferences and values which are diﬃcult to capture by hard- coded reward functions.\\nRLHF works by using a pre-trained LM to generate text, which i s then evaluated by humans by, for example,\\nranking two model generations for the same prompt. This data is then collected to learn a reward model\\nthat predicts a scalar reward given any generated text. The r eward captures human preferences when\\njudging model output. Finally, the LM is optimized against s uch reward model using RL policy gradient\\nalgorithms like PPO ( Schulman et al. ,2017). RLHF can be applied directly on top of a general-purpose LM\\npre-trained via self-supervised learning. However, for mo re complex tasks, the model’s generations may not\\nbe good enough. In such cases, RLHF is typically applied afte r an initial supervised ﬁne-tuning phase using\\na small number of expert demonstrations for the correspondi ng downstream task ( Ramamurthy et al. ,2022;\\nOuyang et al. ,2022;Stiennon et al. ,2020).\\nA successful example of RLHF used to teach a LM to use an extern al tool stems from WebGPT Nakano et al.\\n(2021) (discussed in 3.2.3), a model capable of answering questions using a search engine and providing'), index=0, relevance_score=0.9795897),\n",
       " RerankResponseResultsItem(document=RerankResponseResultsItemDocument(text='We examine the inﬂuence of the amount of RLHF training for two reasons. First, RLHF [13, 57] is an\\nincreasingly popular technique for reducing harmful behaviors in large language models [3, 21, 52]. Some of\\nthese models are already deployed [52], so we believe the impact of RLHF deserves further scrutiny. Second,\\nprevious work shows that the amount of RLHF training can signiﬁcantly change metrics on a wide range of\\npersonality, political preference, and harm evaluations for a given model size [41]. As a result, it is important\\nto control for the amount of RLHF training in the analysis of our experiments.\\n3.2 Experiments\\n3.2.1 Overview\\nWe test the effect of natural language instructions on two related but distinct moral phenomena: stereotyping\\nand discrimination. Stereotyping involves the use of generalizations about groups in ways that are often\\nharmful or undesirable.4To measure stereotyping, we use two well-known stereotyping benchmarks, BBQ\\n[40] (§3.2.2) and Windogender [49] (§3.2.3). For discrimination, we focus on whether models make disparate\\ndecisions about individuals based on protected characteristics that should have no relevance to the outcome.5\\nTo measure discrimination, we construct a new benchmark to test for the impact of race in a law school course'), index=22, relevance_score=0.97548366),\n",
       " RerankResponseResultsItem(document=RerankResponseResultsItemDocument(text='model to estimate the eventual performance of a larger RL policy. The slopes of these lines also\\nexplain how RLHF training can produce such large effective gains in model size, and for example it\\nexplains why the RLHF and context-distilled lines in Figure 1 are roughly parallel.\\n• One can ask a subtle, perhaps ill-deﬁned question about RLHF training – is it teaching the model\\nnew skills or simply focusing the model on generating a sub-distribution of existing behaviors . We\\nmight attempt to make this distinction sharp by associating the latter class of behaviors with the\\nregion where RL reward remains linear inp\\nKL.\\n• To make some bolder guesses – perhaps the linear relation actually provides an upper bound on RL\\nreward, as a function of the KL. One might also attempt to extend the relation further by replacingp\\nKLwith a geodesic length in the Fisher geometry.\\nBy making RL learning more predictable and by identifying new quantitative categories of behavior, we\\nmight hope to detect unexpected behaviors emerging during RL training.\\n4.4 Tension Between Helpfulness and Harmlessness in RLHF Training\\nHere we discuss a problem we encountered during RLHF training. At an earlier stage of this project, we\\nfound that many RLHF policies were very frequently reproducing the same exaggerated responses to all\\nremotely sensitive questions (e.g. recommending users seek therapy and professional help whenever they'), index=13, relevance_score=0.97467697),\n",
       " RerankResponseResultsItem(document=RerankResponseResultsItemDocument(text='We have shown that it’s possible to use reinforcement learning from human feedback to train language models\\nthat act as helpful and harmless assistants. Our RLHF training also improves honesty, though we expect\\nother techniques can do better still. As in other recent works associated with aligning large language models\\n[Stiennon et al., 2020, Thoppilan et al., 2022, Ouyang et al., 2022, Nakano et al., 2021, Menick et al., 2022],\\nRLHF improves helpfulness and harmlessness by a huge margin when compared to simply scaling models\\nup.\\nOur alignment interventions actually enhance the capabilities of large models, and can easily be combined\\nwith training for specialized skills (such as coding or summarization) without any degradation in alignment\\nor performance. Models with less than about 10B parameters behave differently, paying an ‘alignment tax’ on\\ntheir capabilities. This provides an example where models near the state-of-the-art may have been necessary\\nto derive the right lessons from alignment research.\\nThe overall picture we seem to ﬁnd – that large models can learn a wide variety of skills, including alignment, in a mutually compatible way – does not seem very surprising. Behaving in an aligned fashion is just\\nanother capability, and many works have shown that larger models are more capable [Kaplan et al., 2020,'), index=10, relevance_score=0.9641193)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rerank_docs.results[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The impact of reranking on different queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = get_docs(query, top_k=5)\n",
    "rerank_docs = co.rerank(\n",
    "\tquery=query,\n",
    "\tdocuments=list(docs.keys()),\n",
    "\ttop_n=5,\n",
    "\tmodel=\"rerank-english-v3.0\",\n",
    "\treturn_documents=True\n",
    ")\n",
    "docs[rerank_docs.results[0].document.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(query: str, top_k: int, top_n: int):\n",
    "\t# First get vec search results\n",
    "\tdocs = get_docs(query, top_k)\n",
    "\ti2doc = {docs[doc]: doc for doc in list(docs.keys())}\n",
    "\t# Then get rerank results\n",
    "\trerank_docs = co.rerank(\n",
    "\t\tquery=query,\n",
    "\t\tdocuments=list(docs.keys()),\n",
    "\t\ttop_n=top_n,\n",
    "\t\tmodel=\"rerank-english-v3.0\",\n",
    "\t\treturn_documents=True\n",
    "\t)\n",
    "\toriginal_docs = []\n",
    "\treranked_docs = []\n",
    "\t# Compare order change\n",
    "\tfor i, doc in enumerate(rerank_docs.results):\n",
    "\t\trerank_i = docs[doc.document.text]\n",
    "\t\tprint(str(i)+\"\\t->\\t\"+str(rerank_i))\n",
    "\t\tif i != rerank_i:\n",
    "\t\t\treranked_docs.append(f\"[{rerank_i}]\\n\"+doc.document.text)\n",
    "\t\t\toriginal_docs.append(f\"[{i}]\\n\"+i2doc[i])\n",
    "\tfor orig, rerank in zip(original_docs, reranked_docs):\n",
    "\t\tprint(\"ORIGINAL:\\n\"+orig+\"\\n\\nRERANKED:\\n\"+rerank+\"\\n\\n---\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t->\t3\n",
      "1\t->\t0\n",
      "2\t->\t22\n",
      "ORIGINAL:\n",
      "[0]\n",
      "preferences and values which are diﬃcult to capture by hard- coded reward functions.\n",
      "RLHF works by using a pre-trained LM to generate text, which i s then evaluated by humans by, for example,\n",
      "ranking two model generations for the same prompt. This data is then collected to learn a reward model\n",
      "that predicts a scalar reward given any generated text. The r eward captures human preferences when\n",
      "judging model output. Finally, the LM is optimized against s uch reward model using RL policy gradient\n",
      "algorithms like PPO ( Schulman et al. ,2017). RLHF can be applied directly on top of a general-purpose LM\n",
      "pre-trained via self-supervised learning. However, for mo re complex tasks, the model’s generations may not\n",
      "be good enough. In such cases, RLHF is typically applied afte r an initial supervised ﬁne-tuning phase using\n",
      "a small number of expert demonstrations for the correspondi ng downstream task ( Ramamurthy et al. ,2022;\n",
      "Ouyang et al. ,2022;Stiennon et al. ,2020).\n",
      "A successful example of RLHF used to teach a LM to use an extern al tool stems from WebGPT Nakano et al.\n",
      "(2021) (discussed in 3.2.3), a model capable of answering questions using a search engine and providing\n",
      "\n",
      "RERANKED:\n",
      "[3]\n",
      "preferences in order to make it more useful. One key component of RLHF is reward modeling,\n",
      "where the problem is formulated as a regression task to predict a scalar reward given a prompt and\n",
      "a response (Askell et al., 2021; Ouyang et al., 2022). This approach typically requires large-scale\n",
      "comparison data, where two model responses on the same prompt are compared Ouyang et al.\n",
      "(2022). Existing open-source works such as Alpaca, Vicuna, and Dolly (Databricks, 2023) do not\n",
      "involve RLHF due to the high cost of labeling comparison data. Meanwhile, recent studies show that\n",
      "GPT-4 is capable of identifying and ﬁxing its own mistakes, and accurately judging the quality of\n",
      "responses(Peng et al., 2023; Bai et al., 2022; Madaan et al., 2023; Kim et al., 2023). Therefore, to\n",
      "facilitate research on RLHF, we have created comparison data using GPT-4, as described in Section 2.\n",
      "Figure 2: The distribution of comparison\n",
      "data.To evaluate data quality, we train a reward model based\n",
      "on OPT 1.3B (Iyer et al., 2022) to rate different responses. For each instance of the comparison data involving one prompt xandKresponses, GPT-4 assigns a\n",
      "\n",
      "---\n",
      "\n",
      "ORIGINAL:\n",
      "[1]\n",
      "the context, and model-written functions are run in a sandbox environment. In Figure 21 we show results\n",
      "versus model size with and without RLHF training. We see the same trend here as with other evaluations –\n",
      "RLHF decreases the performance of small models, but improves the performance of larger models.\n",
      "RL training tends to decrease the entropy of the models’ distribution, and so we were concerned that these\n",
      "results would be very sensitive to temperature and top-p tuning. So for our 52B models, we performed a\n",
      "scan over temperatures and two top-p settings for both the RLHF models and the base code models, and then\n",
      "chose the best setting for each model and pass@k . We did a grid-search over the evaluation hyperparameters:\n",
      "T2f0;0:4;0:6;0:8;1:0g\u0002p2f0:95;1g\u0002k2f1;5;10;25;50;75;100g. Results are summarized on the\n",
      "right side of Figure 21. For each model and for each kinpass@k , we take the maximum performance over\n",
      "all 10 combinations of hyperparameters. We see that RLHF improves performance over the baseline on this\n",
      "evaluation, for all pass@k .\n",
      "We should emphasize that as with our other evaluations, the improvements in performance from RLHF are\n",
      "\n",
      "RERANKED:\n",
      "[0]\n",
      "preferences and values which are diﬃcult to capture by hard- coded reward functions.\n",
      "RLHF works by using a pre-trained LM to generate text, which i s then evaluated by humans by, for example,\n",
      "ranking two model generations for the same prompt. This data is then collected to learn a reward model\n",
      "that predicts a scalar reward given any generated text. The r eward captures human preferences when\n",
      "judging model output. Finally, the LM is optimized against s uch reward model using RL policy gradient\n",
      "algorithms like PPO ( Schulman et al. ,2017). RLHF can be applied directly on top of a general-purpose LM\n",
      "pre-trained via self-supervised learning. However, for mo re complex tasks, the model’s generations may not\n",
      "be good enough. In such cases, RLHF is typically applied afte r an initial supervised ﬁne-tuning phase using\n",
      "a small number of expert demonstrations for the correspondi ng downstream task ( Ramamurthy et al. ,2022;\n",
      "Ouyang et al. ,2022;Stiennon et al. ,2020).\n",
      "A successful example of RLHF used to teach a LM to use an extern al tool stems from WebGPT Nakano et al.\n",
      "(2021) (discussed in 3.2.3), a model capable of answering questions using a search engine and providing\n",
      "\n",
      "---\n",
      "\n",
      "ORIGINAL:\n",
      "[2]\n",
      "evaluation, for all pass@k .\n",
      "We should emphasize that as with our other evaluations, the improvements in performance from RLHF are\n",
      "modest. In fact, we ﬁnd that simply prompting a base code model performs slightly better, as shown in Figure\n",
      "26\n",
      "1071081091010\n",
      "Number of Parameters0.000.050.100.150.200.250.30Accuracy (pass@1)\n",
      "HumanEval Performance\n",
      "Python FT + RLHF \n",
      "Python FT \n",
      "100101102\n",
      "k in pass@k0.40.50.60.70.80.9Accuracy\n",
      "Effect of RLHF on HumanEval Performance for 52B Models\n",
      "Python FT + RLHF \n",
      "Python FT Figure 21 (left) Pass@1 accuracy of base code models and RLHF models on HumanEval. RLHF generally\n",
      "decreases performance on smaller models, but improves performance on larger models. (right) This ﬁgure\n",
      "shows performance of our 52B models as a function of kfor Pass@k. We did a grid-search over the evaluation\n",
      "hyperparameters T2f0;0:4;0:6;0:8;1:0g\u0002p2f0:95;1g, and plotted the maximum accuracy at each k.\n",
      "Results show that RLHF actually improves performance, even at large k.\n",
      "\n",
      "RERANKED:\n",
      "[22]\n",
      "We examine the inﬂuence of the amount of RLHF training for two reasons. First, RLHF [13, 57] is an\n",
      "increasingly popular technique for reducing harmful behaviors in large language models [3, 21, 52]. Some of\n",
      "these models are already deployed [52], so we believe the impact of RLHF deserves further scrutiny. Second,\n",
      "previous work shows that the amount of RLHF training can signiﬁcantly change metrics on a wide range of\n",
      "personality, political preference, and harm evaluations for a given model size [41]. As a result, it is important\n",
      "to control for the amount of RLHF training in the analysis of our experiments.\n",
      "3.2 Experiments\n",
      "3.2.1 Overview\n",
      "We test the effect of natural language instructions on two related but distinct moral phenomena: stereotyping\n",
      "and discrimination. Stereotyping involves the use of generalizations about groups in ways that are often\n",
      "harmful or undesirable.4To measure stereotyping, we use two well-known stereotyping benchmarks, BBQ\n",
      "[40] (§3.2.2) and Windogender [49] (§3.2.3). For discrimination, we focus on whether models make disparate\n",
      "decisions about individuals based on protected characteristics that should have no relevance to the outcome.5\n",
      "To measure discrimination, we construct a new benchmark to test for the impact of race in a law school course\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare(query, 25, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t->\t2\n",
      "1\t->\t0\n",
      "2\t->\t7\n",
      "ORIGINAL:\n",
      "[0]\n",
      "including limitations and risks that might be exploited by m alicious actors. Further, existing\n",
      "red teaming approaches are insufﬁcient for addressing thes e concerns in the AI context.\n",
      "In order for AI developers to make veriﬁable claims about the ir AI systems being safe or secure, they need\n",
      "processes for surfacing and addressing potential safety an d security risks. Practices such as red teaming\n",
      "exercises help organizations to discover their own limitat ions and vulnerabilities as well as those of the\n",
      "AI systems they develop, and to approach them holistically , in a way that takes into account the larger\n",
      "environment in which they are operating.23\n",
      "A red team exercise is a structured effort to ﬁnd ﬂaws and vuln erabilities in a plan, organization, or\n",
      "technical system, often performed by dedicated \"red teams\" that seek to adopt an attacker’s mindset\n",
      "and methods. In domains such as computer security , red teams are routinely tasked with emulating\n",
      "attackers in order to ﬁnd ﬂaws and vulnerabilities in organi zations and their systems. Discoveries made\n",
      "by red teams allow organizations to improve security and sys tem integrity before and during deployment.\n",
      "Knowledge that a lab has a red team can potentially improve th e trustworthiness of an organization with\n",
      "\n",
      "RERANKED:\n",
      "[2]\n",
      "by red teams allow organizations to improve security and sys tem integrity before and during deployment.\n",
      "Knowledge that a lab has a red team can potentially improve th e trustworthiness of an organization with\n",
      "respect to their safety and security claims, at least to the e xtent that effective red teaming practices exist\n",
      "and are demonstrably employed.\n",
      "As indicated by the number of cases in which AI systems cause o r threaten to cause harm, developers of an\n",
      "AI system often fail to anticipate the potential risks assoc iated with technical systems they develop. These\n",
      "risks include both inadvertent failures and deliberate mis use. Those not involved in the development\n",
      "of a particular system may be able to more easily adopt and pra ctice an attacker’s skillset. A growing\n",
      "number of industry labs have dedicated red teams, although b est practices for such efforts are generally\n",
      "in their early stages.24There is a need for experimentation both within and across or ganizations in order\n",
      "to move red teaming in AI forward, especially since few AI dev elopers have expertise in relevant areas\n",
      "such as threat modeling and adversarial machine learning [44].\n",
      "AI systems and infrastructure vary substantially in terms o f their properties and risks, making in-house\n",
      "red-teaming expertise valuable for organizations with suf ﬁcient resources. However, it would also be\n",
      "\n",
      "---\n",
      "\n",
      "ORIGINAL:\n",
      "[1]\n",
      "future work, we plan on explicitly comparing and contrasting (semi-)manual versus automated approaches\n",
      "to red teaming in order to determine how the two methods vary in the efﬁcacy and diversity of resulting red\n",
      "team attacks.\n",
      "5.2 Policy Interventions\n",
      "Red teaming entails working with inherently controversial subject matter, and most organizations that red\n",
      "team systems have strong counter-incentives to share their ﬁndings.13This is a problem; if we cannot publicly\n",
      "13Red team datasets include offensive content, and may potentially reveal embarrassing or sensitive details about an\n",
      "institution’s AI system if released publicly.\n",
      "14\n",
      "discuss — in detail — how we red team systems and what we learn as a result, it will be difﬁcult to broadly\n",
      "share the future risks, failures, and implications of yet-to-be developed systems. This problem gets worse\n",
      "over time. As systems become more capable, the results of red teaming may surface increasingly undesirable\n",
      "harms. Therefore, we need to change the incentive structure so more organizations share ﬁndings from their\n",
      "red teaming efforts when doing so is safe and beneﬁcial. To do so, we identify two speciﬁc interventions the\n",
      "AI research community could take to build consensus around how to red team andhow to release ﬁndings\n",
      "from red teaming .\n",
      "\n",
      "RERANKED:\n",
      "[0]\n",
      "including limitations and risks that might be exploited by m alicious actors. Further, existing\n",
      "red teaming approaches are insufﬁcient for addressing thes e concerns in the AI context.\n",
      "In order for AI developers to make veriﬁable claims about the ir AI systems being safe or secure, they need\n",
      "processes for surfacing and addressing potential safety an d security risks. Practices such as red teaming\n",
      "exercises help organizations to discover their own limitat ions and vulnerabilities as well as those of the\n",
      "AI systems they develop, and to approach them holistically , in a way that takes into account the larger\n",
      "environment in which they are operating.23\n",
      "A red team exercise is a structured effort to ﬁnd ﬂaws and vuln erabilities in a plan, organization, or\n",
      "technical system, often performed by dedicated \"red teams\" that seek to adopt an attacker’s mindset\n",
      "and methods. In domains such as computer security , red teams are routinely tasked with emulating\n",
      "attackers in order to ﬁnd ﬂaws and vulnerabilities in organi zations and their systems. Discoveries made\n",
      "by red teams allow organizations to improve security and sys tem integrity before and during deployment.\n",
      "Knowledge that a lab has a red team can potentially improve th e trustworthiness of an organization with\n",
      "\n",
      "---\n",
      "\n",
      "ORIGINAL:\n",
      "[2]\n",
      "by red teams allow organizations to improve security and sys tem integrity before and during deployment.\n",
      "Knowledge that a lab has a red team can potentially improve th e trustworthiness of an organization with\n",
      "respect to their safety and security claims, at least to the e xtent that effective red teaming practices exist\n",
      "and are demonstrably employed.\n",
      "As indicated by the number of cases in which AI systems cause o r threaten to cause harm, developers of an\n",
      "AI system often fail to anticipate the potential risks assoc iated with technical systems they develop. These\n",
      "risks include both inadvertent failures and deliberate mis use. Those not involved in the development\n",
      "of a particular system may be able to more easily adopt and pra ctice an attacker’s skillset. A growing\n",
      "number of industry labs have dedicated red teams, although b est practices for such efforts are generally\n",
      "in their early stages.24There is a need for experimentation both within and across or ganizations in order\n",
      "to move red teaming in AI forward, especially since few AI dev elopers have expertise in relevant areas\n",
      "such as threat modeling and adversarial machine learning [44].\n",
      "AI systems and infrastructure vary substantially in terms o f their properties and risks, making in-house\n",
      "red-teaming expertise valuable for organizations with suf ﬁcient resources. However, it would also be\n",
      "\n",
      "RERANKED:\n",
      "[7]\n",
      "red-teaming expertise valuable for organizations with suf ﬁcient resources. However, it would also be\n",
      "beneﬁcial to experiment with the formation of a community of AI red teaming professionals that draws\n",
      "together individuals from different organizations and bac kgrounds, speciﬁcally focused on some subset\n",
      "of AI (versus AI in general) that is relatively well-deﬁned a nd relevant across multiple organizations.25\n",
      "A community of red teaming professionals could take actions such as publish best practices, collectively\n",
      "analyze particular case studies, organize workshops on eme rging issues, or advocate for policies that\n",
      "would enable red teaming to be more effective.\n",
      "Doing red teaming in a more collaborative fashion, as a commu nity of focused professionals across\n",
      "23Red teaming could be aimed at assessing various properties o f AI systems, though we focus on safety and security in this\n",
      "subsection given the expertise of the authors who contribut ed to it.\n",
      "24For an example of early efforts related to this, see Marshall et al., \"Threat Modeling AI /ML Systems and Dependencies\"\n",
      "[43]\n",
      "25In the context of language models, for example, 2019 saw a deg ree of communication and coordination across AI developers\n",
      "to assess the relative risks of different language understa nding and generation systems [10]. Adversarial machine learning,\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare(\"what is red teaming?\", top_k=25, top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(index_name) # Clean up"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
